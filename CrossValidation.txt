
-Train/test validation:
	- train model on train set (70%)
	- evaluate the error on test set(30%):
		- allow to add hyper-parameters
-Train/Test Split procedure:
	0. Clean and adjust data as necessary for both X and y
	1. Split data in Train/Test for both X and y
	2. Fit/Train Scaler on Training X data
	3. Scale X test data
	4. Create model
	5. Fit/Train model on X Train Data
	6. Evaluate Model on X Test data (by creating predictions and comparing to Y_test)
	7. Adjust Paramteres as Necessary and repeat step 5 & 6
	
It is not a fair measure because we adjust hyper-parameters based on previous performance of Test set. So it is not 100% independenly


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = , random_state = )

#Scale data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

#Create model
from sklearn.linear_model import Ridge

model = Ridge(alpha = 100)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

#Evaluate model
from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, y_pred)


------------------------------------------------------------------
 -Split by Train/Validation/Test set
	- performance/fine tuning on train, evaluate on validation
	- get final hyper-parameters --> measure on test set. Test set is final error
	- not adjust after the final test data set is to get the fairest evaliaton. Truly never before seen data
	- performe train_test_split twice. First split for training set, second split remaining data into validation set and test set
	

-Train/Test/Validation Split procedure:
	0. Clean and adjust data as necessary for both X and y
	1. Split data in Train/Validation/Test for both X and y
	2. Fit/Train Scaler on Training X data
	3. Scale X Eval data
	4. Create model
	5. Fit/Train model on X Train Data
	6. Evaluate Model on X evaliaton data (by creating predictions and comparing to Y_eval)
	7. Adjust Paramteres as Necessary and repeat step 5 & 6
	8. Get final metrics on test set (not allowed to go back and adjust after this)
	
	
It is not a fair measure because we adjust hyper-parameters based on previous performance of Test set. So it is not 100% independenly

#using Cross_val_score function
	-start with entire data set
	-split data into Train/test set 
	-remove test set for final evaluation
	-chose K-fold split value for training data
	-Train on K-1 folds and validate on 1 fold
	-avg all errors (pay attention to the different values in different k-folds)
	-get final metrics from final test set

from sklearn.model_selection import cross_val_score, cross_validate
scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error' cv=5)
#https://scikit-learn.org/stable/modules/model_evaluation.html

scores = cross_validate(model, X_train, y_train, scoring = ['neg_mean_absolute_error','neg_mean_squared_error'],cv = 10)	
	
Fit again the model for all training data and then get the final error on final test set

#using Grid search CV

from sklearn.linear_model import ElasticNet
from sklearn.model_selection import GridSearchCV
base_elastic_net_model = ElasticNet()
param_grid = {
	'alpha' = [0.1,1,5,10,50,100]
	'l1_ratio' = [.1,.5,.7,.95,.99,1]
}
grid_model = GridSearchCV(estimator = base_elastic_net_model
						, param_grid = param_grid
						, scoring = 'neg_mean_absolute_error'
						, verbose = 1)
grid_model.fit(X_train, y_train)
grid_model.best_estimator_
pd.DataFrame(grid_model.cv_results_)
y_pred = grid_model.predict(X_test)























